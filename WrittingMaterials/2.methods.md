# Methods

## Universal Parameters
There are a few universal parameters that were used throughout the research to ensure consitency and reproducability across the various statistical methods. The first main parameter is the seed value utilized by multiple methods. The selected seed was $seed=1776$. This seed was used consitently throughout. The second was $k=5$ utilized in the kfold cross validation methods.

## Data Processing
### Data Imputation
Missing values in the rheological dataset were addressed using k-nearest neighbors (KNN) imputation. KNN imputation was selected over alternative approaches due to the limited sample size.

Different from mean or median imputation, which replace missing values with summary statistics, that subsequently ignore the data structure; the approach of KNN imputation preserves the relationships between variables by utilizing information from similar observations. The KNN algorithm identifies the $k$ most similar complete observations based on Euclidean distance across all available features, then imputes missing values using the weighted average of these neighbors' corresponding values. The chosen value of $k=3$ captures local data structure while maintaining robustness against outliers. This approach is well-suited for rheological data, where the parameters often exhibit strong intercorrelations due to their shared dependence.

This non-parametric flexibility is particularly valuable for rheological measurements, which may exhibit complex interdependencies that parametric imputation models might fail to capture adequately.

### Data Standardization
All physiological variables were standardized using sklearn's StandardScaler, which transforms each feature to have zero mean and unit variance according to:
$$
    z_{ij} = \frac{x_{ij}-\mu_j}{\sigma_j}
$$
where $x_{ij}$ represents the original value for sample $i$ and feature $j$, $\mu_j$ is the sample mean, and $\sigma_j$ is the sample standard deviation for feature $j$.

Standardization was essential for the Principal Component Analysis (PCA), as PCA is sensitive to the relative scales of input variables. Without standardization, variables with larger natural scales (e.g., cholesterol levels in mg/dL) would dominate the principal components over variables with smaller scales (e.g., hematocrit percentages), leading to biased dimensionality reduction that reflects measurement units rather than biological relationships.

## Dimensionality Reduction
### Principle Component Analysis
Principal Component Analysis (PCA) was applied to the standardized physiological data to reduce dimensionality while preserving maximum variance. PCA transforms the original feature space into a new coordinate system where the axes (principal components) are linear combinations of the original variables, ordered by the amount of variance they explain.

The mathematical foundation involves eigendecomposition of the covariance matrix:
$$
    C = \frac{1}{n-1} \mathbf{X}^T\mathbf{X}
$$
where $\mathbf{X}$ is the standardized data matrix. The principal components are the eigenvectors of $\mathbf{C}$, and the explained variance for each component corresponds to its eigenvalue.

For the dataset of $n=22$ samples and $25$ physiological variables.

For the purposes of this research the following $13$ features were selected:\
`'HCT', 'FIB', 'CHOL', 'TRIG', 'HDL', 'LDL', 'WBC', 'RBC', 'HEM', 'MCV', 'MCH', 'MCHC'`

The subsequent number of components was $n=22$ samples and $13$ physiological variables, the maximum number of meaningful components is $min(n-1, p)=min(21,13)=13$

### Component Selection
Component selection was guided by several criteria: cumulative explained variance, interpretability of loadings, and downstream modeling performance. The analysis focused primarily on the first $8$ components, which captured the majority of physiological variation while maintaining sufficient sample-to-feature ratios for robust statistical analysis. Principal component loadings were examined to identify which physiological variables contributed most strongly to each component, enabling biological interpretation of the reduced feature space.

## Correlation Analysis
### Pearson Product-Moment Correlations
Pearson product-moment correlation coefficients were calculated to assess linear relationships between principal components and rheological parameters. Pearson correlation was selected as the method for showing viability before advancing to further machine learning techniques.

Both principal components and rheological measurements represent continuous variables measured on interval scales, satisfying the data type requirements for parametric correlation analysis. Principal components are linear combinations of the original physiological variables, making linear correlation analysis theoretically appropriate for detecting associations with rheological behavior.

## Model Validation
### K-Fold
K-fold cross-validation was implemented as the primary methods to validate the Gaussian Process Regression (GPR) analysis. $k=5$ folds was selected to balance the need for adequate training data with robust performance assessment given the limited sample size of $n=22$. This methodology partitions the dataset into five approximately equal segments, with each segment serving as a validation set once while the remaining segments form the training set, repeating this process five times with performance metrics calculated for each iteration. The approach was configured with $n_splits=5$ creating folds containing 4-5 samples each for testing, shuffle=True to randomize sample assignment and prevent systematic bias from potential temporal ordering in data collection or donor recruitment patterns, and $random_state=1776$ to ensure reproducible fold assignments across analysis runs.

K-fold cross-validation helped in for identifying various challenges associated with applying GPR to high-dimensional data with small sample sizes. The cross-validation framework illuminated the severity of overfitting issues inherent in GPR applications to datasets where the sample-to-feature ratio approaches problematic thresholds, as evidenced by the substantial gap between training performance and validation performance across all folds. This diagnostic capability proved instrumental in understanding the limitations of the modeling approach and informed subsequent methodological decisions regarding regularization strategies, with the consistent negative performance across folds providing clear evidence that the challenges were not attributable to specific data partitions or outlier influences but rather represented fundamental mismatches between model complexity and available training data. The stable, reproducible results obtained through the k-fold framework thus served both to rigorously evaluate model performance and to provide critical insights into the broader challenges of machine learning applications in small-sample biological datasets.

Several difficulties in applying GPR to high-dimensional data with small sample numbers were identified with the aid of K-fold cross-validation. The significant difference between training and validation performance across all folds demonstrated the severity of overfitting problems present in GPR applications to datasets where the sample-to-feature ratio approaches problematic thresholds. This was made clear by the cross-validation methodology. This diagnostic ability was crucial in identifying the modeling approach's shortcomings and guided later methodological choices about regularization techniques. The consistently poor performance across folds demonstrated unequivocally that the difficulties were caused by basic inconsistencies between the model's complexity and the training data that was available, rather than by particular data partitions or outlier effects. The reproducibility in the results served as a method to further evelauate the model proformance, providing critical insights into the various challenges of ML applications in fitting a small sample observations in a dataset.

## Machine Learning Methods
### Gaussian Process Regression
Gaussian Process Regression was implemented to model the complex relationships between PCs derived from physiological parameters and individual rheological parameters. GPR was selected specifically for its ability to provide uncertainty quantification, handle non-linear relationships, and perform reasonably well with small datasets.

The GPR framework assumes that the target function $f(x)$ follows a Gaussian process defined by $f(x) \approx GP(m(x), k(x,x^{\prime}))$, where $m(x)$ represents the mean function (set to zero) and $k(x,x^{\prime})$ represents the covariance function that determines similarity between inputs. Combined with k-fold cross-validation, hyperparameter optimization was performed independently for each fold to prevent information leakage, with specific adaptations implemented for small sample robustness including increasing the alpha parameter from typical values of 1e-10 to ranges of 1e-2 to 5.0 to prevent overfitting by adding regularization noise to the kernel matrix diagonal, limiting the number of optimization restarts to 1-2 iterations to prevent hyperparameter overfitting during the search process, and enabling target normalization `(normalize_y=True)` to improve numerical stability and convergence during matrix inversion operations.

The kernel selection strategy employed a composite kernel combining Radial Basis Function and White Noise components according to $k(x,x^\prime)=k_{RBF}(x,x^\prime)+k_{white}(x,x^\prime)$, where bounded constraints on kernel parameters prevented pathological solutions such as memorization of training points or reduction to pure noise models. The RBF kernel captures smooth, non-linear relationships through the mathematical formulation k_RBF(x,x') = σ²_f exp(-|x-x'|²/2ℓ²), where σ²_f represents the signal variance controlling the overall amplitude of function variations, ℓ denotes the length scale parameter determining how quickly correlations decay with distance between inputs, and |x-x'|² represents the squared Euclidean distance between data points in the principal component space.

The White Noise kernel accounts for measurement noise and provides additional regularization through k_White(x,x') = σ²_n δ_ij, where σ²_n represents the noise variance parameter and δ_ij is the Kronecker delta function that equals the noise level when x=x' and zero otherwise, effectively adding independent Gaussian noise to diagonal elements of the covariance matrix. This composite kernel structure enables the model to separate systematic physiological effects captured by the smooth RBF component from random measurement effects modeled by the White Noise component, with hyperparameter bounds set to length_scale_bounds=(1e-2, 1e2) and noise_level_bounds=(1e-5, 1e0) to prevent degenerate solutions while maintaining sufficient flexibility to capture meaningful physiological-rheological relationships.